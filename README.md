# Coursera-GenerativeAI-with-LLMs
## Certificate: https://coursera.org/share/73fd105199bc7bfd78dffc41209969f8


## Week 1:

Explored the concept of model pre-training and established the importance of continued pre-training compared to fine-tuning.\
Defined key terms including Generative AI, large language models, prompt, and detailed the transformer architecture that underpins LLMs.\
Outlined the typical lifecycle of an LLM-based generative AI model, identifying constraining factors influencing decision-making at each stage.\
Analyzed computational challenges encountered during model pre-training and identified strategies for efficiently reducing memory footprint.\
Defined scaling laws and articulated their relevance to LLMs in relation to training dataset size, compute budget, inference requirements, and other pertinent factors.


## Week 2:

Described the benefits of fine-tuning with instructions using prompt datasets, demonstrating performance improvements across various tasks.\
Defined catastrophic forgetting and identified techniques to mitigate its effects.\
Explained the concept of Parameter-efficient Fine Tuning (PEFT) and how it lowers computational costs while addressing catastrophic forgetting.\
Highlighted how fine-tuning with instructions using prompt datasets enhances LLM performance on specific tasks.


## Week 3:

Explained how Reinforcement Learning from Human Feedback (RLHF) leverages human input to enhance the performance and alignment of large language models.\
Detailed the process by which data from human labelers is utilized to train a reward model for RLHF.\
Defined chain-of-thought prompting and illustrated its effectiveness in improving the reasoning and planning abilities of LLMs.\
Discussed the challenges posed by knowledge cut-offs in LLMs and presented information retrieval and augmentation techniques as solutions to these challenges.
